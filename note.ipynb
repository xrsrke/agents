{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'agentml'\n",
    "dataset_id = 'training_data'\n",
    "table_id = 'training_text'\n",
    "\n",
    "table_ref = f\"{project_id}.{dataset_id}.{table_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agentml.training_data.training_text'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = client.get_table(table_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 4, 19, 14, 11, 21, 499529)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-04-19T13:52:28.164730'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = {\n",
    "    \"text\": \"hello 2\",\n",
    "    \"created_at\": datetime.now().isoformat(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = client.insert_rows_json(table, [new_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 0,\n",
       "  'errors': [{'reason': 'invalid',\n",
       "    'location': '',\n",
       "    'debugInfo': '',\n",
       "    'message': 'Missing required fields: Msg_0_CLOUD_QUERY_TABLE.is_deleted, Msg_0_CLOUD_QUERY_TABLE.uuid.'}]}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('de27dc2c-109a-47d3-affe-3aea5b4fc13c')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "uuid.uuid4()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str(uuid.uuid4()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95960628731042371618640198562667613156"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuid.uuid4()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids = [str(uuid.uuid4()) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9fc8b0fb-7dab-44d5-8764-76e603762840',\n",
       " 'cdfc4c5d-7ddf-49bf-9c8f-bcc95309dc2f',\n",
       " '4a0ecbf7-44c9-4912-967b-84cc667e12b0',\n",
       " 'b24ad3e7-bba6-4c9d-be6f-00daaf9f52e3',\n",
       " '08e7397f-5bb8-4383-9880-6bcef57f30f5',\n",
       " 'da05f120-0a88-410b-a785-d4a7f79eb028',\n",
       " '68ba3afe-891e-4471-9e2a-bee6119382c9',\n",
       " 'fe9d30d5-c2dc-4ce4-9a57-adee2ebd7362',\n",
       " 'cc092359-7dc5-44a2-827d-0f8942253bc2',\n",
       " '359f4501-48f0-4a5b-b7ba-5ae4cdaef627']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'9fc8b0fb-7dab-44d5-8764-76e603762840', 'cdfc4c5d-7ddf-49bf-9c8f-bcc95309dc2f', '4a0ecbf7-44c9-4912-967b-84cc667e12b0', 'b24ad3e7-bba6-4c9d-be6f-00daaf9f52e3', '08e7397f-5bb8-4383-9880-6bcef57f30f5', 'da05f120-0a88-410b-a785-d4a7f79eb028', '68ba3afe-891e-4471-9e2a-bee6119382c9', 'fe9d30d5-c2dc-4ce4-9a57-adee2ebd7362', 'cc092359-7dc5-44a2-827d-0f8942253bc2', '359f4501-48f0-4a5b-b7ba-5ae4cdaef627'\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "', '.join(f\"'{id_}'\" for id_ in uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prodgpt.data.data_warehouse import DataWarehouse\n",
    "from prodgpt.utils import load_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_yaml(\"./configs/default.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_warehouse = DataWarehouse(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "uuids = [\"b43b22b8-9d4f-4db9-a639-d754084c2a33\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Table' object has no attribute 'query'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_warehouse\u001b[39m.\u001b[39;49mextract_from_uuid(uuids)\n",
      "File \u001b[0;32m~/DATA/projects/ai/production/AgentProduction/agents/prodgpt/data/data_warehouse.py:50\u001b[0m, in \u001b[0;36mDataWarehouse.extract_from_uuid\u001b[0;34m(self, uuids)\u001b[0m\n\u001b[1;32m     48\u001b[0m str_uuids \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mid_\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m id_ \u001b[39min\u001b[39;00m uuids)\n\u001b[1;32m     49\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSELECT * FROM \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtable_name\u001b[39m}\u001b[39;00m\u001b[39m WHERE uuid IN (\u001b[39m\u001b[39m{\u001b[39;00mstr_uuids\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 50\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtable\u001b[39m.\u001b[39;49mquery(query)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Table' object has no attribute 'query'"
     ]
    }
   ],
   "source": [
    "data_warehouse.extract_from_uuid(uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Table is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_warehouse\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mquery(data_warehouse\u001b[39m.\u001b[39;49mtable)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/google/cloud/bigquery/client.py:3403\u001b[0m, in \u001b[0;36mClient.query\u001b[0;34m(self, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry, api_method)\u001b[0m\n\u001b[1;32m   3392\u001b[0m     \u001b[39mreturn\u001b[39;00m _job_helpers\u001b[39m.\u001b[39mquery_jobs_query(\n\u001b[1;32m   3393\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m   3394\u001b[0m         query,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3400\u001b[0m         job_retry,\n\u001b[1;32m   3401\u001b[0m     )\n\u001b[1;32m   3402\u001b[0m \u001b[39melif\u001b[39;00m api_method \u001b[39m==\u001b[39m enums\u001b[39m.\u001b[39mQueryApiMethod\u001b[39m.\u001b[39mINSERT:\n\u001b[0;32m-> 3403\u001b[0m     \u001b[39mreturn\u001b[39;00m _job_helpers\u001b[39m.\u001b[39;49mquery_jobs_insert(\n\u001b[1;32m   3404\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   3405\u001b[0m         query,\n\u001b[1;32m   3406\u001b[0m         job_config,\n\u001b[1;32m   3407\u001b[0m         job_id,\n\u001b[1;32m   3408\u001b[0m         job_id_prefix,\n\u001b[1;32m   3409\u001b[0m         location,\n\u001b[1;32m   3410\u001b[0m         project,\n\u001b[1;32m   3411\u001b[0m         retry,\n\u001b[1;32m   3412\u001b[0m         timeout,\n\u001b[1;32m   3413\u001b[0m         job_retry,\n\u001b[1;32m   3414\u001b[0m     )\n\u001b[1;32m   3415\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3416\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot unexpected value for api_method: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(api_method)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/google/cloud/bigquery/_job_helpers.py:114\u001b[0m, in \u001b[0;36mquery_jobs_insert\u001b[0;34m(client, query, job_config, job_id, job_id_prefix, location, project, retry, timeout, job_retry)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m         \u001b[39mreturn\u001b[39;00m query_job\n\u001b[0;32m--> 114\u001b[0m future \u001b[39m=\u001b[39m do_query()\n\u001b[1;32m    115\u001b[0m \u001b[39m# The future might be in a failed state now, but if it's\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39m# unrecoverable, we'll find out when we ask for it's result, at which\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39m# point, we may retry.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m job_id_given:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/google/cloud/bigquery/_job_helpers.py:91\u001b[0m, in \u001b[0;36mquery_jobs_insert.<locals>.do_query\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m query_job \u001b[39m=\u001b[39m job\u001b[39m.\u001b[39mQueryJob(job_ref, query, client\u001b[39m=\u001b[39mclient, job_config\u001b[39m=\u001b[39mjob_config)\n\u001b[1;32m     90\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     query_job\u001b[39m.\u001b[39;49m_begin(retry\u001b[39m=\u001b[39;49mretry, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m     92\u001b[0m \u001b[39mexcept\u001b[39;00m core_exceptions\u001b[39m.\u001b[39mConflict \u001b[39mas\u001b[39;00m create_exc:\n\u001b[1;32m     93\u001b[0m     \u001b[39m# The thought is if someone is providing their own job IDs and they get\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[39m# their job ID generation wrong, this could end up returning results for\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[39m# the wrong query. We thus only try to recover if job ID was not given.\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[39mif\u001b[39;00m job_id_given:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/google/cloud/bigquery/job/query.py:1306\u001b[0m, in \u001b[0;36mQueryJob._begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m   1286\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"API call:  begin the job via a POST request\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \n\u001b[1;32m   1288\u001b[0m \u001b[39mSee\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[39m    ValueError: If the job has already begun.\u001b[39;00m\n\u001b[1;32m   1303\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1306\u001b[0m     \u001b[39msuper\u001b[39;49m(QueryJob, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_begin(client\u001b[39m=\u001b[39;49mclient, retry\u001b[39m=\u001b[39;49mretry, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1307\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mGoogleAPICallError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m   1308\u001b[0m     exc\u001b[39m.\u001b[39mmessage \u001b[39m=\u001b[39m _EXCEPTION_FOOTER_TEMPLATE\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1309\u001b[0m         message\u001b[39m=\u001b[39mexc\u001b[39m.\u001b[39mmessage, location\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocation, job_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjob_id\n\u001b[1;32m   1310\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/google/cloud/bigquery/job/base.py:693\u001b[0m, in \u001b[0;36m_AsyncJob._begin\u001b[0;34m(self, client, retry, timeout)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39m# jobs.insert is idempotent because we ensure that every new\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[39m# job has an ID.\u001b[39;00m\n\u001b[1;32m    692\u001b[0m span_attributes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m: path}\n\u001b[0;32m--> 693\u001b[0m api_response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49m_call_api(\n\u001b[1;32m    694\u001b[0m     retry,\n\u001b[1;32m    695\u001b[0m     span_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mBigQuery.job.begin\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    696\u001b[0m     span_attributes\u001b[39m=\u001b[39;49mspan_attributes,\n\u001b[1;32m    697\u001b[0m     job_ref\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    698\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    699\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[1;32m    700\u001b[0m     data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_api_repr(),\n\u001b[1;32m    701\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    702\u001b[0m )\n\u001b[1;32m    703\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_properties(api_response)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/google/cloud/bigquery/client.py:813\u001b[0m, in \u001b[0;36mClient._call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[39mif\u001b[39;00m span_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    810\u001b[0m     \u001b[39mwith\u001b[39;00m create_span(\n\u001b[1;32m    811\u001b[0m         name\u001b[39m=\u001b[39mspan_name, attributes\u001b[39m=\u001b[39mspan_attributes, client\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, job_ref\u001b[39m=\u001b[39mjob_ref\n\u001b[1;32m    812\u001b[0m     ):\n\u001b[0;32m--> 813\u001b[0m         \u001b[39mreturn\u001b[39;00m call()\n\u001b[1;32m    815\u001b[0m \u001b[39mreturn\u001b[39;00m call()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/google/api_core/retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m target \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    346\u001b[0m sleep_generator \u001b[39m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    347\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initial, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maximum, multiplier\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_multiplier\n\u001b[1;32m    348\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m \u001b[39mreturn\u001b[39;00m retry_target(\n\u001b[1;32m    350\u001b[0m     target,\n\u001b[1;32m    351\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predicate,\n\u001b[1;32m    352\u001b[0m     sleep_generator,\n\u001b[1;32m    353\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_timeout,\n\u001b[1;32m    354\u001b[0m     on_error\u001b[39m=\u001b[39;49mon_error,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/google/api_core/retry.py:191\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[39mfor\u001b[39;00m sleep \u001b[39min\u001b[39;00m sleep_generator:\n\u001b[1;32m    190\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m         \u001b[39mreturn\u001b[39;00m target()\n\u001b[1;32m    193\u001b[0m     \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[39m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/site-packages/google/cloud/_http/__init__.py:479\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[39m# Making the executive decision that any dictionary\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39m# data will be sent properly as JSON.\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m data \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m--> 479\u001b[0m     data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mdumps(data)\n\u001b[1;32m    480\u001b[0m     content_type \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mapplication/json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[1;32m    483\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m    484\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     extra_api_info\u001b[39m=\u001b[39mextra_api_info,\n\u001b[1;32m    491\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/json/__init__.py:231\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[39m# cached encoder\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m skipkeys \u001b[39mand\u001b[39;00m ensure_ascii \u001b[39mand\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     check_circular \u001b[39mand\u001b[39;00m allow_nan \u001b[39mand\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m indent \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m separators \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     default \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sort_keys \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 231\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_encoder\u001b[39m.\u001b[39;49mencode(obj)\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/json/encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m encode_basestring(o)\n\u001b[1;32m    196\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[0;32m--> 199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m    201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/json/encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[1;32m    254\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[1;32m    255\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[1;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ml_engineering/lib/python3.8/json/encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[1;32m    161\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \n\u001b[1;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type Table is not JSON serializable"
     ]
    }
   ],
   "source": [
    "data_warehouse.client.query(data_warehouse.table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pf/g3nr86yn4j71vzzmv6knwdhr0000gp/T/ipykernel_11766/1665340599.py:8: MovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  Base = declarative_base()\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, select, and_\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "from sqlalchemy import Column, Integer, String, DateTime, Boolean\n",
    "from typing import Optional, List\n",
    "from datetime import datetime\n",
    "\n",
    "Base = declarative_base()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTable(Base):\n",
    "    __tablename__ = 'training_text'\n",
    "    id = Column(String, primary_key=True)\n",
    "    timestamp_column = Column(DateTime)\n",
    "    is_deleted = Column(Boolean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pf/g3nr86yn4j71vzzmv6knwdhr0000gp/T/ipykernel_11766/2801392009.py:1: SAWarning: This declarative base already contains a class with the same class name and module name as __main__.MyTable, and will be replaced in the string-lookup table.\n",
      "  class MyTable(Base):\n"
     ]
    }
   ],
   "source": [
    "class MyTable(Base):\n",
    "    __tablename__ = 'my_table'\n",
    "    id = Column(String, primary_key=True)\n",
    "    timestamp_column = Column(DateTime)\n",
    "    is_deleted = Column(Boolean)\n",
    "\n",
    "\n",
    "class BigQueryExtractor:\n",
    "    def __init__(self, project_id: str, dataset_id: str):\n",
    "        self.engine = create_engine(f\"bigquery://{project_id}/{dataset_id}\")\n",
    "        self.session = sessionmaker(bind=self.engine)()\n",
    "\n",
    "    def extract(self, uuids: Optional[List[str]] = None, date: Optional[datetime] = None, is_deleted: Optional[bool] = None):\n",
    "        query = select([MyTable])\n",
    "\n",
    "        conditions = []\n",
    "\n",
    "        if uuids:\n",
    "            conditions.append(MyTable.id.in_(uuids))\n",
    "\n",
    "        if date:\n",
    "            date_str = date.strftime('%Y-%m-%d')\n",
    "            conditions.append(MyTable.timestamp_column.cast(\n",
    "                String).like(f\"{date_str}%\"))\n",
    "\n",
    "        if is_deleted is not None:\n",
    "            conditions.append(MyTable.is_deleted == is_deleted)\n",
    "\n",
    "        if conditions:\n",
    "            query = query.where(and_(*conditions))\n",
    "\n",
    "        results = self.session.execute(query)\n",
    "        data = [row for row in results]\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import *\n",
    "from sqlalchemy.engine import create_engine\n",
    "from sqlalchemy.schema import *\n",
    "engine = create_engine('bigquery://agentml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pf/g3nr86yn4j71vzzmv6knwdhr0000gp/T/ipykernel_15191/3575246210.py:1: RemovedIn20Warning: Deprecated API features detected! These feature(s) are not compatible with SQLAlchemy 2.0. To prevent incompatible upgrades prior to updating applications, ensure requirements files are pinned to \"sqlalchemy<2.0\". Set environment variable SQLALCHEMY_WARN_20=1 to show all deprecation warnings.  Set environment variable SQLALCHEMY_SILENCE_UBER_WARNING=1 to silence this message. (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
      "  table = Table('training_data.training_text', MetaData(bind=engine), autoload=True)\n"
     ]
    }
   ],
   "source": [
    "table = Table('training_data.training_text', MetaData(bind=engine), autoload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(select([func.count('*')], from_obj=table).scalar())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
